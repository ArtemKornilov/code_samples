### Задача

Разработать программу, реализующую классификацию документов методом naive bayes. Протестировать программу на дампе stackexchange.

### Использование

``./prog learn --input <input file> --output <stats file>``

``./prog classify --stats <stats file> --stats <stats file> [--stats <stats file> ...] \
--input <input file> --output <output file>``



## Наивный байесовский классификатор.

Наивный байесовский алгоритм, при своей простоте, имеет много применений. Самое прямое – классификация текстов, в частности, проверка письма на спам. Другой пример – оценка тональности текста (проверка рецензии на негативность или позитивность).
Алгоритм имеет в своей основе  теорему Байеса:
![1](https://cloud.githubusercontent.com/assets/7028204/16838249/f6c64762-49d0-11e6-93da-582525aee03d.jpg)

Исходя из нее, можно рассчитать искомую условную вероятность того, что документ d принадлежит классу c. В исходном виде эта теорема не особо применима на практике.  Для ее улучшения предпринимается ряд следующих шагов:

![2](https://cloud.githubusercontent.com/assets/7028204/16838252/f6c86d62-49d0-11e6-965c-c21e6691a843.jpg)

Класс с максимальной апостериорной вероятностью (условной вероятностью, для  которой уже получены из опытов данные) выбирается максимальной величиной среди всех классов из обучающей выборки. Имеем право отбросить знаменатель, который обозначает вероятность документа, т.к. эта вероятность идентична для каждого из классов, а значит не будет влиять на итоговое значение.
В основе следующего шага лежат два предположения. Bag of words assumption: предполагаем что вероятность появления каждого слова в документе не зависит от его положения в тексте. А так же, предположение о том, что вероятность появления каждого слова независима от того, из каких еще слов состоит документ.
 
Именно эти предположения делает байесовский классификатор наивным. Теоретически такое предположение не верно, наивно, но на практике работает:

![3](https://cloud.githubusercontent.com/assets/7028204/16838251/f6c7aa62-49d0-11e6-953b-0c7bdf471fe0.jpg)

Эти два предположения позволяют рассчитать правдоподобие как произведение вероятностей, что сделать значительно проще.
Итоговая формула будет иметь следующий вид:

![4](https://cloud.githubusercontent.com/assets/7028204/16838248/f6c60cca-49d0-11e6-9291-8bd6ae5d3a6c.jpg)

Здесь positions – все слова из текста документа. Вероятность P(c) рассчитывается как количество документов из текущего класса статистического файла, деленое на общее количество
Для расчета правдоподобия (likelihood) существует множество формул, в моем варианте курсовой используется multinomial bayes model:
 
![5](https://cloud.githubusercontent.com/assets/7028204/16838254/f6de37b4-49d0-11e6-836f-8435ef0a85ab.jpg)

Числитель показывает сколько раз слово встречается в документах класса, а знаменатель суммарное количество слов во всех документах этого класса. При таком подходе,  учитывая что все вероятности перемножаются, может возникнуть проблема нигде не встретившегося слова. В таком случае вся вероятность обернется в ноль. Для исключения такого варианта применяется еще один штрих, добавляющий наивности алгоритму: мы предпологаем что каждое слово встречается на один раз больше, это называется аддитивным сглаживанием:


![6](https://cloud.githubusercontent.com/assets/7028204/16838255/f6dfb832-49d0-11e6-8040-d96a4c5282c9.jpg)

 
Проделав все эти шаги, я получил в итоге неверные ответы: из-за сравнительно большого количества данных в статистических файлах у меня выходило переполнение снизу, вероятность обращалась в машинный ноль.
Для такого случая тоже были придуман подход (иначе алгоритм был бы не состоятелен на больших данных): можно считать не сами вероятности, а логарифмы от них по любому основанию. В этом случае изменится только числовое значение, но не итоговый ответ. По свойствам логарифма, при умножении его аргументов, логарифм можно разбить на сумму двух и более, в каждом из которых будет отдельный аргумент:


![7](https://cloud.githubusercontent.com/assets/7028204/16838256/f6e0b7aa-49d0-11e6-8694-51b96e7fad57.jpg)

## Зависимость вероятности успешной классификации от величины выборки

![8](https://cloud.githubusercontent.com/assets/7028204/16838257/f6e0fc9c-49d0-11e6-8a72-2a8f5d92ba66.jpg)
![9](https://cloud.githubusercontent.com/assets/7028204/16838250/f6c75562-49d0-11e6-9301-e1f6e7108a9a.jpg)
![10](https://cloud.githubusercontent.com/assets/7028204/16838253/f6cabea0-49d0-11e6-9703-5a7959f9ee11.png)

Для построения графика я использовал последние 20000 строк из следующих разделов:
-	math и mathoverflow;
-	ux и musiс;
-	opensource, biology и aviation

